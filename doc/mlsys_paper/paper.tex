\documentclass{article}
\usepackage[preprint]{nips_2018}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{array}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{wasysym}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage[newfloat]{minted}
\usepackage{caption}
\usepackage{parcolumns}
\usepackage{booktabs}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{adjustbox}

\newcommand*\OK{\ding{51}}

\newcolumntype{R}[2]{%
  >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
  l%
  <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{30}{2.0em}}}

\setlength{\textfloatsep}{0.1cm}

\begin{document}

% TODO: title could possibly be improved
%\title{\texttt{ensmallen}: a generic C++ library for fast optimization}  %% it's not clear what "optimization" refers to here
%% other possibilities:
%\title{\texttt{ensmallen}: a flexible C++ library for function optimization in machine learning}
%\title{\texttt{ensmallen}: a fast C++ library for function optimization in machine learning}
%\title{\texttt{ensmallen}: a C++ library for fast function optimization in machine learning}
%\title{\texttt{ensmallen}: a C++ library for fast and flexible function optimization}
%\title{\texttt{ensmallen}: a C++ library of fast and flexible function optimizers}
%\title{\texttt{ensmallen}: a library of flexible function optimizers in C++}
%\title{\texttt{ensmallen}: a library of fast and flexible function optimizers in C++}
%\title{\texttt{ensmallen}: a flexible C++ library for function optimization}
\title{\texttt{ensmallen}: a flexible C++ library for efficient function optimization}

% Alphabetical ordering?
% TODO: check affiliations
\author{Shikhar Bhardwaj \\
Delhi Technological University \\
Delhi, India 110042 \\
\texttt{shikhar\_bt2k15@dtu.ac.in}
\And
Ryan R. Curtin \\
RelationalAI \\
Atlanta, GA, USA 30318 \\
\texttt{ryan@ratml.org}
\And
Marcus Edel \\
Free University of Berlin \\
Arnimallee 7, 14195 Berlin \\
\texttt{marcus.edel@fu-berlin.de}
\And
Yannis Mentekidis
% any affiliation/email?
\And
Conrad Sanderson \\
Data61, CSIRO \\
Australia
}

\maketitle

\begin{abstract}
\vspace*{-0.3em}
%% the abstract below still needs more meat and sharpening
We present {\tt ensmallen}, a fast and flexible C++ library for mathematical optimization of 
arbitrary user-supplied functions,
which can be applied to many machine learning problems.
Several types of optimizations are supported, including differentiable,
separable, constrained, and categorical objective functions.
The library provides many pre-built optimizers
(including numerous variants of SGD and Quasi-Newton optimizers)
as well as a flexible framework for implementing new optimizers and objective functions.
Implementation of a new optimizer requires only one method
and a new objective function requires at most four C++ functions. 
This can aid in the quick implementation and prototyping of new machine learning algorithms.
Due to the use of C++ template metaprogramming, {\tt ensmallen} is able to
support compiler optimizations that provide fast runtimes.
Empirical comparisons show that {\tt ensmallen} is able to outperform other
optimization frameworks (such as Julia and SciPy), sometimes by large margins.
The library is distributed under the 3-clause BSD license and is ready for use
in production environments.
\end{abstract}

\section{Introduction}

Mathematical optimization is the workhorse of virtually all machine learning
algorithms.  For a given objective function $f(\cdot)$
(which may have a special structure or constraints),
almost all machine learning problems can be boiled down
to the following optimization form:
%
%\vspace*{-0.2em}
\begin{equation}
\operatorname{argmin} f(x)
\end{equation}
\vspace*{-1.5em}

Optimization is often computationally intensive and may correspond
to most of the time taken to train a machine learning model.  For instance, the
training of deep neural networks is dominated by the optimization of the model
parameters on the data~\cite{schmidhuber2015deep}.
Even popular machine learning models such as logistic regression
have training times mostly dominated by an optimization~\cite{kingma2015adam}.
% TODO: might be nice to have something kind of anecdotal like 'even new
% students to the field of machine learning quickly encounter optimization' and
% cite, e.g., Andrew Ng's coursera course or some ML textbook or similar
%% CS: i think we don't need to explore this too much; better to cut out all the
%% CS: fat and stick with concrete examples, instead of veering off on tangents
%
% or maybe just a note about how many optimization techniques get published at
% NIPS every year?
%% CS: NIPS is too self-referential here

The ubiquity of optimization in machine learning algorithms highlights the need
for robust and flexible implementations of optimization algorithms.
We present {\tt ensmallen}, a C++ optimization toolkit
that contains a wide variety of optimization techniques for many types of
objective functions.  Through the use of C++ template
metaprogramming~\cite{Alexandrescu2001},
{\tt ensmallen} is able to generate efficient code that can help with the
demanding computational needs of many machine learning algorithms.

Although there are many existing machine learning optimization toolkits, few
are able to take explicit advantage of metaprogramming based code optimizations,
and few offer robust support for various types of objective functions.
For instance, deep learning
libraries like Caffe~\cite{jia2014caffe},
PyTorch~\cite{paszke2017automatic},
and TensorFlow~\cite{abadi2016tensorflow}
each contain a variety of optimization techniques.  However, these techniques are
limited to stochastic gradient descent (SGD) and SGD-like optimizers that
operate on small batches of data points at a time.  Other machine learning
libraries, such as {\tt scikit-learn}~\cite{pedregosa2011scikit}
contain optimization algorithms but not in a coherent or reusable framework.
Many programming languages have higher-level packages for
mathematical optimization.  For example, {\tt
scipy.optimize}~\cite{jones2014scipy},
is widely used in the Python community, and MATLAB's function optimization
support has been available and used for many decades.
However, these
implementations are often unsuitable for modern machine learning tasks---for
instance, computing the full gradient of the objective function may not be
feasible because there are too many data points.

In this paper, we describe the functionality of {\tt ensmallen} and the types of
problems that it can be applied to.  We discuss the mechanisms by which {\tt ensmallen} is
able to provide both computational efficiency and ease-of-use.
We show a few examples that use the library, as well as empirical performance comparisons
with other optimization libraries.

{\tt ensmallen} is open-source software licensed under the 3-clause BSD
license\footnote{\url{https://opensource.org/licenses/BSD-3-Clause}},
allowing unencumbered use in both open-source and proprietary projects.
It is available for download from \url{https://www.ensmallen.org}.
Armadillo~\cite{sanderson2016armadillo} is used for efficient linear algebra operations.

\vspace*{-0.2em}
\section{Types of objective functions}
\vspace*{-0.6em}

\begin{table}[b!]
\vspace*{-1.0em}
\centering
    \begin{tabular}{@{} cl*{7}c @{}}
%  \begin{tabular}{ccccccc}
        & & \multicolumn{7}{c}{} \\[0.6ex]
            % If there is any coherent framework at all, this is true.
        & & \rot{unified framework}
            % If there is any support for constrained optimization, this is
            % true.
          & \rot{constraints}
            % If the optimization framework can do mini-batch, this is true.
          & \rot{batches}
            % If I can implement any arbitrary function to be optimized, this is
            % true.
          & \rot{arbitrary functions}
            % If I can implement any new optimization technique to use, this is
            % true.
          & \rot{arbitrary optimizers}
            % If the framework could take advantage of when the gradient is
            % sparse, this is true.
          & \rot{sparse gradients}
            % If the framework can handle categorical/discrete variables for
            % optimization, this is true.
          & \rot{categorical} \\
        \cmidrule{2-9}
  \hline
        % It might be reasonable to say mlpack categorical support is only
        % partial, but I am not sure exactly where we draw the line.
        & {\tt ensmallen}            & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE \\
        % The Shogun toolbox has a fairly nice framework, but it doesn't support
        % sparse gradients or categorical features.  It also does not appear to
        % support constraints.
        & Shogun \cite{sonnenburg2010shogun}             & \CIRCLE & - & \CIRCLE & \CIRCLE & \CIRCLE & - & -
\\
        % VW doesn't appear to have any framework whatsoever and the code is
        % awful, but it does support batches and categorical features.
        & Vowpal Wabbit \cite{Langford2007VW}      & - & - & \CIRCLE  & - & - & - &
\CIRCLE \\
        % TensorFlow has a few optimizers, but they are all SGD-related.  You
        % can write most objectives easily (but some very hard), and categorical
        % support might be possible but would not be easy.
        & TensorFlow \cite{abadi2016tensorflow}        & \CIRCLE & -  & \CIRCLE  & \LEFTcircle & - &
\LEFTcircle & -  \\
        % Caffe has a nice framework, but it's only for SGD-related optimizers.
        % I think I could write a new one, but it is not the easiest thing in
        % the world.
        & Caffe \cite{jia2014caffe}           & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - \\
        % Keras is restricted to neural networks and SGD-like optimizers.  I
        % don't know that it is possible to easily write a new optimizer.
        & Keras \cite{chollet2015}            & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - \\
        % sklearn has a few optimizer frameworks, but they are all in different
        % places and have somewhat different support.
        & scikit-learn \cite{pedregosa2011scikit}       & \LEFTcircle & - & \LEFTcircle  & \LEFTcircle & -
& - & - \\
        % scipy has some nice optimizer framework but it does not support
        % batches or some of the more complex functionality.  And you can't
        % write your own.
        & SciPy \cite{jones2014scipy}             & \CIRCLE & \CIRCLE  & -  & \CIRCLE & - & - & - \\
        % MATLAB is very similar to scipy.
        & MATLAB \cite{mathworks2017OTB}            & \CIRCLE & \CIRCLE & - & \CIRCLE & - & - & - \\
        % Optim.jl isn't the only Julia package for optimization, but it's the
        % one we compare against.
        & Julia ({\tt Optim.jl}) \cite{julia}         & \CIRCLE & - & - & \CIRCLE & - & - & - \\
        \cmidrule[1pt]{2-9}
    \end{tabular}
%   \begin{tablenotes}\footnotesize
\caption{\footnotesize{
Feature comparison: \CIRCLE = provides feature,
\LEFTcircle = partially provides feature, - = does not provide feature.
{\it unified framework} indicates if there is some kind of generic/unified
optimization framework; {\it constraints} and {\it batches} indicate support for
constrained problems and batches; {\it arbitrary functions} means arbitrary
objective functions are easily implemented; {\it arbitrary optimizers} means
arbitrary optimizers are easily implemented; {\it sparse gradient} indicates
that the framework can natively take advantage of sparse gradients; and
{\it categorical} refers to if support for categorical features exists.
}}
\label{tab:functionality}
\vspace*{-1.8em}
\end{table}

{\tt ensmallen} provides a {\bf set of optimizers} for
{\bf optimizing user-defined objective functions}.  It's also easy to implement a
new optimizer in the {\tt ensmallen} framework.  Overall, our goal is to provide
an easy-to-use library that can solve the problem
%\vspace*{-0.4em}
%\begin{equation}
$\operatorname{argmin}_{x} f(x)$
%\end{equation}
%\vspace*{-0.4em}
%\noindent
for any function $f(x)$ that takes some vector or matrix input $x$.  Ideally,
the user should only need to specify what $f(x)$ is (or provide an
implementation of a function that can compute $f(x)$).  But it is impossible to
implement something both so generic and fast---for instance, gradient-based
approaches converge far more quickly than gradient-free approaches (in general),
so we must design an abstraction that is able to simultaneously generalize to
many problem types, as well as take advantage of accelerations and
optimizations.

Therefore, users may implement objective functions with specific properties that
the library can use to accelerate the computation.  These non-exclusive
properties are shown below:

\vspace*{-0.3em}
\begin{itemize} \itemsep -1pt
  \item {\bf arbitrary}: no assumptions can be made on $f(x)$
  \item {\bf differentiable}: $f(x)$ has a computable gradient $f'(x)$
  \item {\bf separable}: $f(x)$ is a sum of individual components: $f(x) =
\sum_{i} f_i(x)$
  \item {\bf categorical}: $x$ contains elements that can only take discrete
values
  %\item {\bf numeric}: all elements of $x$ take values in $\mathcal{R}$
  \item {\bf sparse}: the gradient $f'(x)$ or $f_i(x)$ (for a separable
function) is sparse
  \item {\bf partially differentiable}: the gradient $f'_j(x)$ is computable for
individual elements $x_j$ of $x$
  \item {\bf bounded}: $x$ is limited in the values that it can take
\end{itemize}
\vspace*{-0.3em}

In this framework, if a user wants to optimize a differentiable objective
function, they need only provide implementations of $f(x)$ and $f'(x)$, and
then they can use any of the gradient-based optimizers that {\tt ensmallen}
provides.  Table~\ref{tab:functionality} compares
the classes of objective functions that {\tt ensmallen} can handle with other
popular frameworks and libraries.

Of course, not every optimization algorithm provided by {\tt ensmallen} can be
used by every class of objective function; for instance, a gradient-based
optimizer such as L-BFGS cannot operate on a non-differentiable objective
function.

Therefore, the best the library can attain is to maximize the flexibility
available, so that a user can easily implement a function $f(x)$ and have it
work with as many optimizers as possible.
To do this, {\tt ensmallen} makes heavy use of C++ template metaprogramming.
When implementing an objective function to be
optimized, a user can implement only a few methods and we can use
metaprogramming to check that the given functions match the requirements of the
optimizer that is being used.  When implementing an optimizer, we can assume
that the given function to be optimized meets the required assumptions of the
optimizers, and encode those requirements as \texttt{static\_assert} checks.

For the most common case of a differentiable $f(x)$, the user need only
implement two methods:

\vspace*{-0.3em}
\begin{itemize} \itemsep -1pt
  \item \texttt{double Evaluate($x$)}: given coordinates $x$, this function
should return the value of $f(x)$.
  \item \texttt{void Gradient($x$, $g$)}: given coordinates $x$ and a reference
to $g$, set $g = f'(x)$.
\end{itemize}
\vspace*{-0.3em}

Or, alternately, they can simply implement a \texttt{EvaluateWithGradient()}
function that computes both $f(x)$ and $f'(x)$ simultaneously, which is useful
in cases where both the objective and gradient depend on similar computations.

The required API for separable differentiable objective functions (i.e. those
that would use an optimizer like SGD) is very similar, except that
\texttt{Evaluate()} and \texttt{Gradient()} (or \texttt{EvaluateWithGradient()})
should operate only on small mini-batches.  The same pattern applies for other
types of objective functions: only a few methods specific to class of objective
function itself must be implemented and then any optimizer may be used.

\vspace*{-0.3em}
\section{Example: Learning Linear Regression Models}
\vspace*{-0.5em}

As an example of usage, consider the linear regression objective
function\footnote{Just for simplicity, we ignore the bias term.  It can be
rederived by taking $x^*_i = (x_i, 1)$.}.  Given some dataset $X \in
\mathcal{R}^{n \times d}$ and associated responses $y \in \mathcal{R}^n$, the
model of linear regression is to assume that $y_i = x_i \theta$ for each
point and response $(x_i, y_i)$.  To fit this model $\theta \in \mathcal{R}^d$ best
to the data, we must find

\vspace*{-1.1em}
\begin{equation}
\operatorname{argmin}_\theta \sum_{i = 1}^n (y_i - x_i \theta)^2 =
\operatorname{argmin}_\theta \| y - X \theta \|_F^2.
\end{equation}
\vspace*{-1.1em}

This objective function $f(\theta)$ has the associated gradient

\vspace*{-1.1em}
\begin{equation}
f'(\theta) = \sum_{i = 1}^n -2 x_i (y_i - x_i \theta) = -2 X^T (y - X \theta).
\end{equation}
\vspace*{-1.1em}

We can implement these two functions in a class {\tt LinearRegressionFunction}.
Here {\tt arma::mat} and {\tt arma::vec} represent Armadillo matrix and vector
types, respectively~\cite{sanderson2016armadillo}.

% Indented a little to make it clear it's different from the text...
\vspace*{-0.4em}
\begin{minted}[fontsize=\small]{c++}
    class LinearRegressionFunction {
     public:
      // Construct the LinearRegressionFunction with the given data.
      LinearRegressionFunction(arma::mat& X, arma::vec& y) : X(X), y(y) {}

      // Compute the objective function.
      double Evaluate(const arma::mat& theta) {
        return arma::accu((y - X * theta) % (y - X * theta));
      }
      // Compute the gradient and store in 'gradient'.
      void Gradient(const arma::mat& theta, arma::mat& gradient) {
        gradient = -2 * X.t() * (y - X * theta);
      }
     private:
      arma::mat& X; arma::vec& y;
    };
\end{minted}
\vspace*{-0.4em}

This is all the implementation that is required to optimize this function with
any of {\tt ensmallen}'s gradient-based optimizers.  The code snippet below
shows how L-BFGS could be used to find the best parameters $\theta$:

\vspace*{-0.4em}
\begin{minted}[fontsize=\small]{c++}
    LinearRegressionFunction lrf(X, y); // we assume X and y are loaded elsewhere
    ens::L_BFGS lbfgs; // create L-BFGS optimizer with default parameters

    arma::vec theta(X.n_rows, arma::fill::randu); // random uniform initialization
    lbfgs.Optimize(lrf, theta); // after this call, theta holds the solution
\end{minted}
\vspace*{-0.4em}

Only a slight variation on the signature of {\tt Evaluate()} and {\tt
Gradient()} would be needed to use the small-batch SGD-like optimizers that {\tt
ensmallen} provides.

\vspace*{-0.3em}
\section{Available optimizers}
\vspace*{-0.5em}

Due to the easy abstraction, {\tt ensmallen} is able to provide a large set of
diverse optimization algorithms.  Below is a list of what is currently
available.

% I guess to save some space we should group them.
\vspace*{-0.4em}
\begin{itemize}
  \item {\bf SGD variants:} Stochastic Gradient Descent (SGD), Stochastic
      Coordinate Descent (SCD), Parallel Stochastic Gradient Descent (Hogwild!),
      Stochastic Gradient Descent with Restarts (SGDR), SMORMS3, AdaGrad,
      AdaDelta, RMSProp, Adam, AdaMax

  \item {\bf Quasi-Newton variants:} Limited-memory BFGS (L-BFGS), incremental
        Quasi-Newton method (IQN), Augmented Lagrangian Method

  \item {\bf Genetic variants:} Conventional Neuro-evolution (CNE), Covariance
        Matrix Adaptation Evolution Strategy (CMA-ES)

  \item {\bf Other:} Conditional Gradient Descent, Frank-Wolfe algorithm, Simulated Annealing

% These were a part of mlpack but not ensmallen.
  %\item {\bf Objective functions:} Neural Networks, Logistic regression,
  %    Matrix completion, Neighborhood Components Analysis, Regularized SVD,
  %    Reinforcement learning, Softmax regression, Sparse autoencoders,
  %    Sparse SVM
\end{itemize}
\vspace*{-0.4em}

In addition, many optimization algorithms are currently in development and will
be released in the future.

\vspace*{-0.3em}
\section{Templates for speed}
\vspace*{-0.5em}

Many optimization toolkits depend on the user passing in a function pointer to a
function that needs to be optimized.  For instance, the {\tt scipy.optimize}
package in Python can be used like this:

\vspace*{-0.3em}
\begin{minted}[fontsize=\footnotesize]{python}
    def rosen(x): return 100.0 * (x[2] - x[1]**2.0)**2.0 + (1 - x[1])**2.0)
    res = scipy.optimize.minimize(rosen, [-1.2, 1], method='nelder-mead')
\end{minted}
\vspace*{-0.3em}

During the call to {\tt minimize()}, the {\tt rosen()} function will be called
repeatedly---but it will be called as a function pointer, which must be
dereferenced each time {\tt rosen()} is called.  In addition, some
optimizations such as inlining (and many optimizations enabled by inlining) are
not possible when using function pointers.  Since some optimization algorithms
need to compute the objective very many times, the overhead of the lookup and
missed optimization opportunities may be non-negligible.  This applies even for
SGD-like optimizers that train machine learning models on very large datasets!

Since {\tt ensmallen} is written in C++ with templates, the function pointer
dereferencing overhead can be avoided and the compiler can choose to inline the
function call and perform additional optimizations when possible, such as
temporary variable elimination and dead code pruning.

\vspace*{-0.3em}
\section{Automatic generation of methods}
\vspace*{-0.5em}

It is often the case that when optimizing a function $f(x)$, the computation of
the value $f(x)$ and its derivative $f'(x)$ involve the computation of identical
intermediate results.  Consider the linear regression objective function from
earlier:
%\begin{equation}
$f(\theta) = \| y - X\theta \|_F^2.$
%\end{equation}

For this objective function, the derivative $f'(x)$ is the similar $-2 X^T (y -
X \theta)$, and both depend on the computation of the vector term $(y - X
\theta)$.  If $X$ is a large matrix, then this may be computationally expensive
to compute.  Existing optimization frameworks do not have an easy way to avoid
this duplicate computation; however, in many cases, an optimization algorithm
may need the values of both $f(\theta)$ and $f'(\theta)$ for a given $\theta$.

Using template metaprogramming, {\tt ensmallen} provides an easy (and
optional) way for users to avoid this extra computational overhead.  Instead of
specifying individual {\tt Evaluate()} and {\tt Gradient()} functions, a user
may simply write an {\tt EvaluateWithGradient()} function that returns both the
objective value and the gradient value for an input $\theta$.  For our earlier
\texttt{LinearRegressionFunction}, we could use an implementation that only
computes $(y - X \theta)$ once:

\vspace*{-0.5em}
\begin{minted}[fontsize=\small]{c++}
    double EvaluateWithGradient(const arma::mat& theta, arma::mat& gradient) {
      const arma::rowvec v = (y - X * theta); // Cache result.
      gradient = -2 * X.t() * v;
      return arma::accu(v % v); // Take squared norm of v.
    }
\end{minted}
\vspace*{-0.5em}

% This is up here so that it's on the right page...
\begin{table}[t]
\begin{center}
\begin{tabular}{cccc}
\toprule
{\tt ensmallen} & {\tt scipy} & {\tt Optim.jl} & {\tt samin} \\
\midrule
% TODO: these are just single-run results from Marcus' laptop!  We need to do
% 10 and average.
{\bf 0.004s} & 1.044s & 0.021s & 2.920s \\
\bottomrule
\end{tabular}
\end{center}
\caption{Runtimes for $100000$ iterations of simulated annealing with different
frameworks on the simple Rosenbrock function.  Julia code runs do not count
compilation time.}
\label{tab:rosenbrock_results}
\end{table}

This method could be provided {\it instead} of {\tt Evaluate()} and {\tt
Gradient()}.  In either case, template metaprogramming techniques are used to
detect which methods exist, and a wrapper class will use suitable mix-ins in
order to provide any `missing` functionality~\cite{smaragdakis2000mixin}.  For
instance, if {\tt EvaluateWithGradient()} is not provided, a version will be
generated that calls both {\tt Gradient()} and {\tt Evaluate()} in turn.
Similarly, if {\tt Evaluate()} or {\tt Gradient()} does not exist, then {\tt
EvaluateWithGradient()} is called, and the unnecessary part of the result will
be discarded.

Since all of this code generation is done at compile-time and not
at runtime, the compiler is able to perform dead code elimination
on the unused result, and may be able to avoid calculations entirely.  In some
cases, then, the generated {\tt Evaluate()} or {\tt Gradient()} functions may be
equivalently fast to what would be hand-written!

Overall, this code generation functionality reduces the requirements for users
when they are implementing their own objective functions to be optimized.  At
the time of this writing, this is implemented the most commonly-used cases:
full-batch and small-batch {\tt Evaluate()}, {\tt Gradient()}, and {\tt
EvaluateWithGradient()}.  In the future, this support may be expanded to other
sets of methods for other types of objective functions.

% TODO: anything to write about the visualization page that we had set up?

\vspace*{-0.3em}
\section{Experiments}
\vspace*{-0.5em}

To demonstrate the benefits of the metaprogramming based code optimizations
that {\tt ensmallen} can exploit,
we compare the performance of {\tt ensmallen} with several other
optimization frameworks, including some that use automatic differentiation.

First, we consider the added overhead of other frameworks.  For our experiment,
we use the simple and popular Rosenbrock function~\cite{Rosenbrock1960}:
$f([x_1, x_2]) = 100 (x_2 - x_1^2)^2 + (1 - x_1^2)$.  For the optimizer, we use
simulated annealing~\cite{kirkpatrick1983optimization},
a gradient-free optimizer.  Simulated annealing will call the objective function
numerous times; for each simulation we limit the optimizer to 100K
objective evaluations.  We compare four frameworks%
%
\footnote{Another option here might be {\tt simulannealbnd()} 
in the Global Optimization Toolkit for MATLAB.
However, no license was available for these simulations.}
%
for this task:

\vspace*{-0.3em}
\begin{itemize} \itemsep -1pt
  \item {\tt ensmallen}
  \item {\tt scipy.optimize.anneal}, from scipy 0.14.1~\cite{jones2014scipy}
  \item simulated annealing implementation in {\tt Optim.jl} with Julia
1.0.1~\cite{mogensen2018optim}
  \item {\tt samin} in the {\tt optim} package for GNU Octave~\cite{octave}
\end{itemize}
\vspace*{-0.3em}

Table~\ref{tab:rosenbrock_results} shows the average runtime
across 10 trials for 100K iterations of simulated annealing with each
implementation.  The simulations were run on a % TODO: what are Marcus's
% system specs?
{\tt \$COMPUTER}.
Only Julia and {\tt ensmallen} are able to avoid the
function pointer dereference and take advantage of inlining and related
optimizations---and this difference is strongly reflected in the results.

%% CS: i'm not convinced that this conjecture is barking up the right tree;
%% CS: SciPy is in Python, which is by default an interpreted language.
%% CS: the slowdown many have nothing to do with inlining and simply due to
%% CS: Python being slow in general.
%% CS: it's possible to use PyPy or Numba to speed things up,
%% CS: in order to provide a "fairer" comparison against compiled languages.
%% CS: Octave also has an optional JIT compiler, which last time I checked
%% CS: needs to be explicitly enabled.
%% 
%% CS: i would suggest to stay away from the "we do inlining" argument,
%% CS: as it can be blown up too easily.
%% CS: furthermore, I would suggest to mention PyPy and/or Numba and Octave JIT
%% CS: (in order to show that we've done our homework),
%% CS: and then state something along the lines of
%% CS: "these approaches are not being used as we're comparing out-of-the-box performance"

Next, we consider the linear regression example first described in Section~\ref{sec:linreg_example}.
For this task we use the first-order L-BFGS optimizer~\cite{zhu1997algorithm},
so for {\tt ensmallen} we will implement two versions: one with {\tt Evaluate()}
and {\tt Gradient()}, and one with only {\tt EvaluateWithGradient()}.  The code
for each of these is the same as shown earlier.  In addition, we have more
options for Julia: we can implement only the objective function and allow either
{\tt Calculus.jl}\footnote{\url{https://github.com/JuliaMath/Calculus.jl}} or {\tt
ForwardDiff.jl}~\cite{RevelsLubinPapamarkou2016} packages to automatically
compute the gradient.  Overall, we compare against the following:

\vspace*{-0.3em}
\begin{itemize} \itemsep -1pt
  \item {\tt ensmallen}: only {\tt EvaluateWithGradient()} implemented
  \item {\tt ensmallen}-2: both {\tt Evaluate()} and {\tt Gradient()} separately
implemented
  \item {\tt Calculus.jl}: uses the default {\tt Calculus.jl} for
computing the gradient with {\tt Optim.jl}
  \item {\tt ForwardDiff.jl}: uses {\tt ForwardDiff.jl} for computing
the gradient with {\tt Optim.jl}
  \item {\tt Optim.jl}: implements the gradient manually
  \item {\tt scipy}: implements both objective and
gradient
  \item {\tt bfgsmin}: uses {\tt optim} package for GNU Octave
\end{itemize}
\vspace*{-0.3em}

Results for various data sizes are shown in Table~\ref{tab:lbfgs}.  For each
implementation, L-BFGS was allowed to run for only $10$ iterations and never
converged in fewer iterations.  The datasets used for training are highly noisy random
data with a slight linear pattern. Note that the exact data is not relevant
for the experiments here, only its size.  Runtimes are again reported as the
average across 10 runs.

\begin{table}
\begin{center}
\begin{tabular}{lccccc}
\toprule
{\em algorithm} & $d$: 100, $n$: 1k & $d$: 100, $n$: 10k & $d$: 100, $n$:
100k & $d$: 1k, $n$: 100k \\
\midrule
% TODO: this was only one trial on Ryan's desktop!
{\tt ensmallen} & {\bf 0.001s} & {\bf 0.006s} & {\bf 0.139s} & {\bf 1.698s} \\
{\tt ensmallen}-2 & 0.002s & 0.008s & 0.204s & 2.497s \\
{\tt Calculus.jl} & 0.172s & 0.960s & 27.428s & 2535.507s \\
{\tt ForwardDiff.jl} & 0.553s & 1.162s & 6.665s & 778.250s \\
{\tt Optim.jl} & 0.007s & 0.025s & 0.390s & 4.107s \\
{\tt scipy.optimize} & 0.002s & 0.010s & 0.198s & 2.022s \\
{\tt bfgsmin} & 0.215s & 1.705s & 15.998s & ??? \\
\bottomrule
\end{tabular}
\end{center}
\caption{Runtimes for linear regression function with various dataset sizes,
with $n$ indicating the number of samples,
and $d$ indicating the dimensionality of each sample.
All Julia runs do not count compilation time.}
\label{tab:lbfgs}
\end{table}

The results indicate that {\tt ensmallen} with {\tt EvaluateWithGradient()}
is the fastest approach.
Furthremore, the use of {\tt EvaluateWithGradient()} yields considerable
speedup of almost 1.5x over the {\tt ensmallen} implementation with both the
objective and gradient implemented separately.  In addition, although the
automatic differentiation support makes it easier for the user to write their
code, it is clear that the code generated by the automatic differentiators
({\tt Calculus.jl} and {\tt ForwardDiff.jl})
is far from optimal, with runs that are slower by up to three orders of magnitude.

\vspace*{-0.3em}
\section{Conclusion}
\vspace*{-0.5em}

We have described {\tt ensmallen}, a flexible C++ library for function
optimization that provides an easy interface for users to implement and optimize
their desired objective functions.  Many types of functions can be optimized,
including separable and constrained functions.  The library is fast due to its
use of template metaprogramming and the Armadillo matrix library for linear
algebra.  In addition, template metaprogramming is used to make user
implementation easier by automatically generating missing methods.

GPU support for optimization is available either through the use of the NVBLAS
library~\cite{nvidia2015} or the forthcoming Bandicoot C++ GPU matrix library.
% Conrad: is it ok if we mention that here?  It's a really nice selling point
% and I am sure at least one reviewer will be thinking about GPUs.

{\tt ensmallen} is already in use as the optimization toolkit of the {\tt
mlpack} machine learning library~\cite{mlpack2018}.  For more information on the
toolkit, see the website and documentation at \url{https://www.ensmallen.org/}.

\subsubsection*{Acknowledgements}

The development team of {\tt ensmallen} does not include just the authors named
here but also a long list of other contributors.  See
\url{https://www.ensmallen.org/about.html} for more information.
% TODO: that URL may change

\bibliographystyle{plain}
\bibliography{paper}

\end{document}

\documentclass{article}
\usepackage{nips_2018}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{array}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{wasysym}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage[newfloat]{minted}
\usepackage{caption}
\usepackage{parcolumns}
\usepackage{booktabs}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{adjustbox}

\newcommand*\OK{\ding{51}}

\newcolumntype{R}[2]{%
  >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
  l%
  <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{30}{2em}}}

\begin{document}

% TODO: title could possibly be improved
\title{\texttt{ensmallen}: a generic C++ library for fast optimization}

% Alphabetical ordering?
\author{Shikhar Bhardwaj \And Ryan R. Curtin \And Marcus Edel \And Yannis
Mentekidis \And Conrad Sanderson}

\maketitle

\begin{abstract}

%% the abstract below still needs more meat and sharpening

We present a fast and flexible C++ library for mathematical optimization of 
arbitrary user-supplied functions,
which can be applied to many machine learning problems.
The library provides many pre-built optimizers
(including numerous variants of SGD and Quasi-Newton optimizers)
as well as a flexible framework for implementing new optimizers and objective functions.
Implementation of a new optimizer requires only one method
and implementation of a new objective function requires at most four C++ functions. 
This can aid in the  quick implementation and prototyping of new machine learning algorithms.
In contrast to optimization approaches in existing frameworks
(such as Julia, Shogun, Vowpal Wabbit, SciPy),
the proposed Ensmallen library supports more optimizations,
more types of objective functions,
as well as seamlessly supporting user-defined objective functions and optimizers.
Empirical comparisons show that ...
TODO: briefly describe the speedups obtained in the experiments


\end{abstract}

\section{Introduction}

Mathematical optimization is the workhorse of virtually all machine learning
algorithms.  For a given objective function $f(\cdot)$
(which may have a special structure or constraints),
almost all machine learning problems can be boiled down
to the following optimization form:
%
\begin{equation}
\operatorname{argmin} f(x)
\end{equation}

Optimization is often computationally intensive and may correspond
to most of the time taken to train a machine learning model.  For instance, the
training of deep neural networks is dominated by the optimization of the model
parameters on the data~\cite{schmidhuber2015deep}.
Even popular machine learning models such as logistic regression
have training times mostly dominated by an optimization~\cite{kingma2015adam}.
% TODO: might be nice to have something kind of anecdotal like 'even new
% students to the field of machine learning quickly encounter optimization' and
% cite, e.g., Andrew Ng's coursera course or some ML textbook or similar
%% CS: i think we don't need to explore this too much; better to cut out all the
%% CS: fat and stick with concrete examples, instead of veering off on tangents
%
% or maybe just a note about how many optimization techniques get published at
% NIPS every year?
%% CS: NIPS is too self-referential here

The ubiquity of optimization in machine learning algorithms highlights the need
for robust and flexible implementations of optimization algorithms.
We present {\tt ensmallen}, a C++ optimization toolkit
that contains a wide variety of optimization techniques for many types of
objective functions.  Through the use of C++ template metaprogramming~\cite{TODO},
{\tt ensmallen} is able to generate efficient code that can help with the
demanding computational needs of many machine learning algorithms.

Although there are many existing machine learning optimization toolkits, few
are able to take explicit advantage of metaprogramming based code optimizations,
and few offer robust support for various types of objective functions.
For instance, deep learning
libraries like Caffe~\cite{jia2014caffe},
PyTorch~\cite{paszke2017automatic},
and TensorFlow~\cite{abadi2016tensorflow}
each contain a variety of optimization techniques.  However, these techniques are
limited to stochastic gradient descent (SGD) and SGD-like optimizers that
operate on small batches of data points at a time.  Other machine learning
libraries, such as {\tt scikit-learn}~\cite{pedregosa2011scikit}
% TODO: can we find another library to put here too?  Shark maybe?
contain optimization algorithms but not in a coherent or reusable framework.
Many programming languages have higher-level packages for
mathematical optimization.  For example, {\tt scipy.optimize} % TODO: check and cite
is widely used in the Python community, and MATLAB's function optimization
support has been available and used for many decades.  However, these
implementations are often unsuitable for modern machine learning tasks---for
instance, computing the full gradient of the objective function may not be
feasible because there are too many data points.

In this paper, we describe the functionality of {\tt ensmallen} and the types of
problems that it can be applied to.  We discuss the mechanisms by which {\tt ensmallen} is
able to provide both computational efficiency and ease-of-use.
We show a few examples that use the library, as well as empirical performance comparisons
with other optimization libraries.

{\tt ensmallen} is open-source software licensed under the 3-clause BSD
license\footnote{\url{https://opensource.org/licenses/BSD-3-Clause}},
allowing unencumbered use in both open-source and proprietary projects.
It is available for download from \url{https://www.ensmallen.org}.
Armadillo~\cite{sanderson2016armadillo} is used for efficient linear algebra operations.

% Originally, {\tt ensmallen} was developed as part of {\tt mlpack}; % TODO: cite
% it is now available in standalone header-only form.
% TODO: seems awkward to put the lineage here;
%% CS: agreed; i don't see a need to explicitly mention mlpack -- it distracts
%% CS: from the central message
% TODO: also, we should talk up the use

\section{Types of objective functions}

In short, {\tt ensmallen} provides a {\bf set of optimizers} that can be used to
{\bf optimize user-defined objective functions}.  It's also easy to implement a
new optimizer in the {\tt ensmallen} framework.  Overall, our goal is to provide
an easy-to-use library that can solve the problem

\begin{equation}
\operatorname{argmin}_{x} f(x)
\end{equation}

\noindent for any function $f(x)$ that takes some vector or matrix input $x$.  Ideally,
the user should only need to specify what $f(x)$ is (or provide an
implementation of a function that can compute $f(x)$).  But it is impossible to
implement something both so generic and fast---for instance, gradient-based
approaches converge far more quickly than gradient-free approaches (in general),
so we must design an abstraction that is able to simultaneously generalize to
many problem types, as well as take advantage of accelerations and
optimizations.

Therefore, users may implement objective functions with specific properties that
the library can use to accelerate the computation.  These non-exclusive
properties are shown below:

\begin{itemize}
  \item {\bf arbitrary}: no assumptions can be made on $f(x)$
  \item {\bf differentiable}: $f(x)$ has a computable gradient $f'(x)$
  \item {\bf separable}: $f(x)$ is a sum of individual components: $f(x) =
\sum_{i} f_i(x)$
  \item {\bf categorical}: $x$ contains elements that can only take discrete
values
  %\item {\bf numeric}: all elements of $x$ take values in $\mathcal{R}$
  \item {\bf sparse}: the gradient $f'(x)$ or $f_i(x)$ (for a separable
function) is sparse
  \item {\bf partially differentiable}: the gradient $f'_j(x)$ is computable for
individual elements $x_j$ of $x$
  \item {\bf bounded}: $x$ is limited in the values that it can take
\end{itemize}

In this framework, if a user wants to optimize a differentiable objective
function, they simply need to provide implementations of $f(x)$ and $f'(x)$, and
then they can use any of the gradient-based optimizers that {\tt ensmallen}
provides to perform the optimization.  Table~\ref{tab:functionality} compares
the classes of objective functions that {\tt ensmallen} can handle with other
popular frameworks and libraries.

% Just to get a feeling of the look; Also, we could also just use \OK instead of
% \CIRCLE or \LEFTcircle
\begin{table}
\centering
    \begin{tabular}{@{} cl*{7}c @{}}
%  \begin{tabular}{ccccccc}
        & & \multicolumn{7}{c}{} \\[2ex]
            % If there is any coherent framework at all, this is true.
        & & \rot{unified framework}
            % If there is any support for constrained optimization, this is
            % true.
          & \rot{constraints}
            % If the optimization framework can do mini-batch, this is true.
          & \rot{batches}
            % If I can implement any arbitrary function to be optimized, this is
            % true.
          & \rot{arbitrary functions}
            % If I can implement any new optimization technique to use, this is
            % true.
          & \rot{arbitrary optimizers}
            % If the framework could take advantage of when the gradient is
            % sparse, this is true.
          & \rot{sparse gradients}
            % If the framework can handle categorical/discrete variables for
            % optimization, this is true.
          & \rot{categorical} \\
        \cmidrule{2-9}
  \hline
        % It might be reasonable to say mlpack categorical support is only
        % partial, but I am not sure exactly where we draw the line.
        & {\tt ensmallen}            & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE \\
        % The Shogun toolbox has a fairly nice framework, but it doesn't support
        % sparse gradients or categorical features.  It also does not appear to
        % support constraints.
        & Shogun \cite{sonnenburg2010shogun}             & \CIRCLE & - & \CIRCLE & \CIRCLE & \CIRCLE & - & -
\\
        % VW doesn't appear to have any framework whatsoever and the code is
        % awful, but it does support batches and categorical features.
        & Vowpal Wabbit \cite{Langford2007VW}      & - & - & \CIRCLE  & - & - & - &
\CIRCLE \\
        % TensorFlow has a few optimizers, but they are all SGD-related.  You
        % can write most objectives easily (but some very hard), and categorical
        % support might be possible but would not be easy.
        & TensorFlow \cite{abadi2016tensorflow}        & \CIRCLE & -  & \CIRCLE  & \LEFTcircle & - &
\LEFTcircle & -  \\
        % Caffe has a nice framework, but it's only for SGD-related optimizers.
        % I think I could write a new one, but it is not the easiest thing in
        % the world.
        & Caffe \cite{jia2014caffe}           & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - \\
        % Keras is restricted to neural networks and SGD-like optimizers.  I
        % don't know that it is possible to easily write a new optimizer.
        & Keras \cite{chollet2015}            & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - \\
        % sklearn has a few optimizer frameworks, but they are all in different
        % places and have somewhat different support.
        & scikit-learn \cite{pedregosa2011scikit}       & \LEFTcircle & - & \LEFTcircle  & \LEFTcircle & -
& - & - \\
        % scipy has some nice optimizer framework but it does not support
        % batches or some of the more complex functionality.  And you can't
        % write your own.
        & SciPy \cite{jones2014scipy}             & \CIRCLE & \CIRCLE  & -  & \CIRCLE & - & - & - \\
        % MATLAB is very similar to scipy.
        & MATLAB \cite{mathworks2017OTB}            & \CIRCLE & \CIRCLE & - & \CIRCLE & - & - & - \\
        % TODO: Julia
        % TODO: MATLAB paper from MLOSS 2018?
        \cmidrule[1pt]{2-9}
    \end{tabular}
%   \begin{tablenotes}\footnotesize
\caption{
Feature comparison: \CIRCLE = provides feature,
\LEFTcircle = partially provides feature, - = does not provide feature.
{\it unified framework} the library/language has some kind of generic and unified
optimization framework; {\it constraints} and {\it batches} indicate support for
constrained problems and batches; {\it arbitrary functions} means arbitrary
objective functions are easily implemented; {\it arbitrary optimizers} means
arbitrary optimizers are easily implemented; {\it sparse gradient} indicates
that the framework can natively take advantage of sparse gradients; and
{\it categorical} refers to whether support for categorical features exists.
}
\label{tab:functionality}
\end{table}

Of course, not every optimization algorithm provided by {\tt ensmallen} can be
used by every class of objective function; for instance, a gradient-based
optimizer such as L-BFGS cannot operate on a non-differentiable objective
function.

Therefore, the best we can hope to achieve is to maximize the flexibility
available, so that a user can easily implement a function $f(x)$ and have it
work with as many optimizers as possible.  For this, {\tt ensmallen} depends on
C++ policy-based design.  When implementing an objective function to be
optimized, a user can implement only a few methods and we can use C++ template
metaprogramming to check that the given functions match the requirements of the
optimizer that is being used.  When implementing an optimizer, a user can assume
that the given function to be optimized meets the required assumptions of the
optimizers, and encode those requirements as \texttt{static\_assert} checks.

For the most common case of a differentiable objective function, the user must
implement only two methods:

\begin{itemize}
  \item \texttt{double Evaluate($x$)}: given coordinates $x$, this function
should return the value of $f(x)$.
  \item \texttt{void Gradient($x$, $g$)}: given coordinates $x$ and a reference
to $g$, set $g = f'(x)$.
\end{itemize}

Or, alternately, they can simply implement a \texttt{EvaluateWithGradient()}
function that computes both $f(x)$ and $f'(x)$ simultaneously, which is useful
in cases where both the objective and gradient depend on similar computations.

The required API for separable differentiable objective functions (i.e. those
that would use an optimizer like SGD) is very similar, except that
\texttt{Evaluate()} and \texttt{Gradient()} (or \texttt{EvaluateWithGradient()})
should operate only on small mini-batches.  The same pattern applies for other
types of objective functions: only a few methods specific to class of objective
function itself must be implemented and then any optimizer may be used.

\section{Example}

As an example of usage, let us consider the linear regression objective
function\footnote{Just for simplicity, we ignore the bias term.  It can be
rederived by taking $x^*_i = (x_i, 1)$.}.  Given some dataset $X \in
\mathcal{R}^{n \times d}$ and associated responses $y \in \mathcal{R}^n$, the
model of linear regression is to assume that $y_i = x_i \theta$ for each data
point and response $(x_i, y_i)$.  To fit this model $\theta \in \mathcal{R}^d$ best
to the data, we wish to minimize

\begin{equation}
\operatorname{argmin}_\theta \sum_{i = 1}^n (y_i - x_i \theta)^2 =
\operatorname{argmin}_\theta \| y - X \theta \|^2.
\end{equation}

This objective function $f(\theta)$ has the associated gradient

\begin{equation}
f'(\theta) = \sum_{i = 1}^n -2 x_i (y_i - x_i \theta) = -2 X^T (y - X \theta).
\end{equation}

We can implement these two functions in a class {\tt LinearRegressionFunction}.
Here {\tt arma::mat} and {\tt arma::vec} represent Armadillo matrix and vector
types, respectively~\cite{sanderson2016armadillo}.

% TODO: syntax highlighting
\begin{verbatim}
class LinearRegressionFunction
{
 public:
  // Construct the LinearRegressionFunction with the given data.
  LinearRegressionFunction(arma::mat& X, arma::vec& y) : X(X), y(y) {}

  // Compute the objective function.
  double Evaluate(const arma::mat& theta)
  {
    return arma::accu((y - X * theta) % (y - X * theta));
  }

  // Compute the gradient and store in 'gradient'.
  void Gradient(const arma::mat& theta, arma::mat& gradient)
  {
    gradient = -2 * X.t() * (y - X * theta);
  }

 private:
  arma::mat& X;
  arma::vec& y;
};
\end{verbatim}

This is all the implementation that is required to optimize this function with
any of {\tt ensmallen}'s gradient-based optimizers.  The code snippet below
shows how L-BFGS could be used to find the best parameters $\theta$:

\begin{verbatim}
LinearRegressionFunction lrf(X, y); // we assume X and y are loaded elsewhere
ens::L_BFGS lbfgs; // create L-BFGS optimizer with default parameters

arma::vec theta(X.n_rows, arma::fill::randu); // random uniform initialization
lbfgs.Optimize(lrf, theta); // after this call, theta holds the solution
\end{verbatim}

% TODO if space: elaborate on the example to add SGD-like optimizers.  The code
% is a little more complex but not much.

\section{Available optimizers}

Thanks to the easy abstraction, we have been able to provide support for a large
set of diverse optimizers and objective functions.  Below is a list of what is
currently available.

% I guess to save some space we should group them.
\begin{itemize}
  \item {\bf SGD variants:} Stochastic Gradient Descent (SGD), Stochastic
      Coordinate Descent (SCD), Parallel Stochastic Gradient Descent (Hogwild!),
      Stochastic Gradient Descent with Restarts (SGDR), SMORMS3, AdaGrad,
      AdaDelta, RMSProp, Adam, AdaMax

  \item {\bf Quasi-Newton variants:} Limited-memory BFGS (L-BFGS), incremental
        Quasi-Newton method (IQN), Augmented Lagrangian Method

  \item {\bf Genetic variants:} Conventional Neuro-evolution (CNE), Covariance
        Matrix Adaptation Evolution Strategy (CMA-ES)

  \item {\bf Other:} Conditional Gradient Descent, Frank-Wolfe algorithm, Simulated Annealing

  \item {\bf Objective functions:} Neural Networks, Logistic regression,
      Matrix completion, Neighborhood Components Analysis, Regularized SVD,
      Reinforcement learning, Softmax regression, Sparse autoencoders,
      Sparse SVM
\end{itemize}

In addition, many methods are currently in development and will be released in
the future.  % TODO: maybe some more clarity here

% not even sure this is the right term
\section{Static polymorphism for speed}

% TODO: better transition from the previous section

Many optimization toolkits depend on the user passing in a function pointer to a
function that needs to be optimized.  For instance, the {\tt scipy.optimize}
package in Python can be used like this:

%% TODO: code snippet


During the optimization, the {\tt rosen()} function will be called repeatedly.
However, since what was given to the {\tt minimize()} function is a function
pointer, this pointer must be dereferenced at each call.  In addition, some
optimizations such as inlining (and many optimizations enabled by inlining) are
not possible when using function pointers.  Because it is possible that an
individual optimization routine may need to compute the objective function,
gradient, and related quantities many times, the overhead of this lookup may be
non-negligible.  As one example, when training neural networks with SGD-like
optimizers on a dataset with $N$ points, the gradient must be repeatedly
computed on mini-batches of points of size $b$, and the training will pass over
the whole dataset $e$ times.  For typical values of $N \sim 100M$, $b = 64$, and
$e = 100$, this means the gradient computation function is being called almost
200 million times!

Since {\tt ensmallen} is written in C++ with templates, the function pointer
dereferencing overhead can be avoided and the compiler can choose to inline the
function call and perform additional optimizations when possible, such as
temporary variable elimination and dead code pruning.
% TODO: check that these make sense
% TODO: needs a reference

\section{Automatic Creation of Functions}

It is often the case that when optimizing a function $f(x)$, the computation of
the value $f(x)$ and its derivative $f'(x)$ involve the computation of identical
intermediate results.  Consider the linear regression objective function from
earlier:

\begin{equation}
f(\theta) = \| y - X\theta \|_F^2.
\end{equation}

For this objective function, the derivative $f'(x)$ is the similar $-2 X^T (y -
X \theta)$, and both depend on the computation of the vector term $(y - X
\theta)$.  If $X$ is a large matrix, then this may be computationally expensive
to compute.  Existing optimization frameworks do not have an easy way to avoid
this duplicate computation; however, in many cases, an optimization algorithm
may need the values of both $f(\theta)$ and $f'(\theta)$ for a given $\theta$.

Using template metaprogramming in {\tt ensmallen}, we provide an easy (and
optional) way for users to avoid this extra computational overhead.  Instead of
specifying individual {\tt Evaluate()} and {\tt Gradient()} functions, a user
may simply write an {\tt EvaluateWithGradient()} function that returns both the
objective value and the gradient value for an input $\theta$.  For our earlier
\texttt{LinearRegressionFunction}, we could use an implementation that only
computes $(y - X \theta)$ once:

\begin{verbatim}
double EvaluateWithGradient(const arma::mat& theta, arma::mat& gradient)
{
  const arma::rowvec v = (y - X * theta); // Cache result.
  gradient = -2 * X.t() * v;
  return arma::accu(v % v); // Take squared norm of v.
}
\end{verbatim}

This method could be provided {\it instead} of {\tt Evaluate()} and {\tt
Gradient()}.

In essence, given a {\tt FunctionType} class that implements either an {\tt
Evaluate()} and {\tt Gradient()} method or an {\tt EvaluateWithGradient()}
method, template metaprogramming techniques are used to detect which of these
methods exist.  Then, a wrapper class will use suitable mix-ins
in order to provide the `missing' functionality~\cite{smaragdakis2000mixin}.  If
{\tt EvaluateWithGradient()} does not exist, the following code is generated:

% TODO: code block
\begin{verbatim}
double EvaluateWithGradient(const arma::mat& coordinates, arma::mat& gradient)
{
  Gradient(coordinates, gradient);
  return Evaluate(coordinates);
}
\end{verbatim}

Similarly, if {\tt Evaluate()} or {\tt Gradient()} does not exist, then {\tt
EvaluateWithGradient()} is called, and the unnecessary part of the result will
be discarded.  Since all of this code generation is done at compile-time and not
at runtime, the compiler is able to perform dead code elimination % TODO: check
on the unused result, and may be able to avoid calculations entirely.  In some
cases, then, the generated {\tt Evaluate()} or {\tt Gradient()} functions may be
equivalently fast to what would be hand-written!

Overall, this code generation functionality reduces the requirements for users
when they are implementing their own objective functions to be optimized.  At
the time of this writing, this is implemented the most commonly-used cases:
full-batch and small-batch {\tt Evaluate()}, {\tt Gradient()}, and {\tt
EvaluateWithGradient()}.  In the future, this support may be expanded to other
sets of methods for other types of objective functions.

% TODO: anything to write about the visualization page that we had set up?

\section{Experiments}

To demonstrate the benefits of these optimizations that {\tt ensmallen} can take
advantage of, we compare {\tt ensmallen}'s performance to a number of other
optimization frameworks, including some that use automatic differentiation.

First, we consider the simple and popular Rosenbrock
function~\cite{Rosenbrock1960}:

\begin{equation}
f([x_1 x_2]) = 100 (x_2 - x_1^2)^2 + (1 - x_1^2).
\end{equation}

To demonstrate the added overhead of other frameworks, we choose to use
simulated annealing~\cite{kirkpatrick1983optimization},
a gradient-free optimizer that only uses the objective function itself.
Simulated annealing will call the objective function very many times; for each
simulation we limit the optimizer to $100,000$ objective evaluations.  We
compare four frameworks for this task:

\begin{itemize}
  \item {\tt ensmallen}
  \item {\tt scipy.optimize.anneal}, from scipy 0.14.1~\cite{jones2014scipy}
  \item {\tt Optim.jl}'s simulated annealing implementation with Julia
1.0.1~\cite{mogensen2018optim}
  \item {\tt samin} from GNU Octave's {\tt optim} package~\cite{octave}
\footnote{Another option here might be {\tt simulannealbnd()} from MATLAB's
Global Optimization Toolkit.  However, no license was available for these
simulations.}
\end{itemize}

Table~\ref{tab:rosenbrock_results} shows the average runtime
across 10 trials for $100000$ iterations of simulated annealing with each
implementation.  Of these frameworks, only Julia and {\tt ensmallen} are able to
avoid the function pointer dereference and take advantage of inlining and
related optimizations---and this difference is strongly reflected in the
results.

\begin{table}[t]
\begin{center}
\begin{tabular}{cccc}
\toprule
{\tt ensmallen} & {\tt scipy} & {\tt Optim.jl} & {\tt samin} \\
\midrule
% TODO: these are just single-run results from Marcus' laptop!  We need to do
% 10 and average.
{\bf 0.004s} & 1.044s & 0.021s & 2.920s \\
\bottomrule
\end{tabular}
\end{center}
\caption{Runtimes for $100000$ iterations of simulated annealing with different
frameworks on the simple Rosenbrock function.  Julia code runs do not count
compilation time.}
\label{tab:rosenbrock_results}
\end{table}

Next, we consider the same machine learning example we have been working with
through the paper: linear regression.  For this task we will use the first-order
L-BFGS optimizer~\cite{zhu1997algorithm},
so for {\tt ensmallen} we will implement two versions: one with {\tt Evaluate()}
and {\tt Gradient()}, and one with only {\tt EvaluateWithGradient()}.  The code
for each of these is the same as shown earlier.  In addition, we have more
options for Julia: we can implement only the objective function and allow either
Julia's {\tt
Calculus.jl}\footnote{\url{https://github.com/JuliaMath/Calculus.jl}} or {\tt
ForwardDiff.jl}~\cite{RevelsLubinPapamarkou2016} package to automatically
compute the gradient.  Therefore, we compare against the following:

\begin{itemize}
  \item {\tt ensmallen}: only {\tt EvaluateWithGradient()} implemented
  \item {\tt ensmallen}-2: both {\tt Evaluate()} and {\tt Gradient()} separately
implemented
  \item {\tt Optim.jl+Calculus.jl}: uses the default {\tt Calculus.jl} for
computing the gradient
  \item {\tt Optim.jl+ForwardDiff.jl}: uses {\tt ForwardDiff.jl} for computing
the gradient
  \item {\tt Optim.jl}: implements the gradient manually
  \item {\tt scipy.optimize.fmin\_l\_bfgs\_b}: implements both objective and
gradient
  \item {\tt bfgsmin} from GNU Octave's {\tt optim} package
\end{itemize}

Results for different data sizes are shown in Table~\ref{tab:lbfgs}.  For each
implementation, L-BFGS was allowed to run for only $10$ iterations and never
converged in fewer iterations.  The datasets trained on were highly noisy random
data with a slight linear pattern---but note that the exact data is not relevant
for our experiments here, only its size.  Runtimes are again reported as the
average across 10 runs.

\begin{table}
\begin{center}
\begin{tabular}{lccccc}
\toprule
{\em algorithm} & $d = 100$, $n = 1k$ & $d = 100$, $n = 10k$ & $d = 100$, $n =
100k$ & $d = 1k$, $n = 100k$ \\
\midrule
% TODO: this was only one trial on Ryan's desktop!
{\tt ensmallen} & {\bf 0.001s} & {\bf 0.006s} & {\bf 0.139s} & {\bf 1.698s} \\
{\tt ensmallen}-2 & 0.002s & 0.008s & 0.204s & 2.497s \\
{\tt Optim.jl+Calculus.jl} & 0.172s & 0.960s & 27.428s & 2535.507s \\
{\tt Optim.jl+ForwardDiff.jl} & 0.553s & 1.162s & 6.665s & 778.250s \\
{\tt Optim.jl} & 0.007s & 0.025s & 0.390s & 4.107s \\
{\tt scipy.optimize.fmin\_l\_bfgs\_b} & 0.002s & 0.010s & 0.198s & 2.022s \\
{\tt bfgsmin} & 0.215s & 1.705s & 15.998s & ??? \\
\bottomrule
\end{tabular}
\end{center}
\caption{Runtimes for linear regression function with several different dataset
sizes.  All Julia runs do not count compilation time.}
\label{tab:lbfgs}
\end{table}

We can see that the use of {\tt EvaluateWithGradient()} yields considerable
speedup of almost 1.5x over the {\tt ensmallen} implementation with both the
objective and gradient implemented separately.  In addition, although the
automatic differentiation support makes it easier for the user to write their
code, it is very clear that the code generated by the automatic differentiator
is far from optimal, with runs that take up to three orders of magnitude longer!

\section{Conclusion}

Here we can just summarize everything we've been talking about and possibly
point towards things that can happen in the future:

\begin{itemize}
  \item GPU support via Bandicoot eventually (although NVBLAS can already be
used)
  \item Additional optimizers added
  \item Anything else?
\end{itemize}

\subsubsection*{Acknowledgements}

The development of {\tt ensmallen} does not include just the authors named
here but also a long list of other contributors.  See \url{%TODO about page
} for more information.

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
